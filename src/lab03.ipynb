{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d944382",
   "metadata": {},
   "source": [
    "## Laboratorio 03 - Programación dinámica\n",
    "Integrantes:\n",
    "- Ricardo Méndez\n",
    "- Sara Echevería\n",
    "- Melissa Pérez\n",
    "\n",
    "Repositorio: https://github.com/MelissaPerez09/Lab03-CC3104"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be156223",
   "metadata": {},
   "source": [
    "### Task 01\n",
    "1. ¿Qué es Programación Dinámica y cómo se relaciona con RL?\n",
    "    - La programación dinámica es una técnica de optimización que se usa para resolver problemas complejos. Lo que hace es dividir el problema en subproblemas para hacerlo más simple y así poder resolver uno a la vez.\n",
    "    - Se relaciona con RL porque es aplicable en los casos donde el modelo conoce el entorno completo para buscar una política óptima. Principalmente se relacionan los conceptos de la iteración valor y la iteración de póliza.\n",
    "2. Explique en sus propias palabras el algoritmo de Iteración de Póliza.\n",
    "    - Es un algoritmo que busca encontrar la política óptima en un MDP mediante un proceso iterativo. El proceso se basa en la evaluación y la mejora de la política.\n",
    "    - Primero, se parte de una política aleatoria donde en la evaluación se calcula una función de valor esperado para definir cada estado al seguir la política. Luego, en la mejora se ajusta la política usando la función de valor obtenida para seleccionar en cada estado la acción que maximice ese valor. El ciclo se repite iterativamente hasta obtener la política óptima.\n",
    "3. Explique en sus propias palabras el algoritmo de Iteración de Valor.\n",
    "    - Es una técnica de programación dinámica para calcular el valor óptimo de cada estado y la política óptima de manera simultánea. \n",
    "    - Se inicia con una función valor con ceros donde se va actualizando con la ecuación de Bellman. En cada estado se elige la acción que maximice la suma de la recompensa esperada. El proceso se repite hasta que los valores de los estados dejen de cambiar para que al final, se extraiga la política óptima.\n",
    "4. En el laboratorio pasado, vimos que el valor de los premios obtenidos se mantienen constantes, ¿por qué?\n",
    "    - Se mantenían constantes porque estaban definidas como parte del entorno del MDP, no dependían de la política ni de los valores de los estados. Básicamente, no importaba la secuencia de decisiones porque era un ambiente determinista con una recompensa fija.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e1cef5",
   "metadata": {},
   "source": [
    "### Task 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae0a3b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ecb2f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#entorno\n",
    "grid_size = 3\n",
    "\n",
    "states = list(range(grid_size * grid_size))\n",
    "\n",
    "actions = {\n",
    "    0: \"up\",\n",
    "    1: \"down\",\n",
    "    2: \"left\",\n",
    "    3: \"right\"\n",
    "}\n",
    "\n",
    "start = 0\n",
    "goal = 8\n",
    "obstacles = [2, 4, 6]\n",
    "gamma = 0.9\n",
    "\n",
    "#recompensas\n",
    "reward_goal = 10\n",
    "reward_obstacle = -10\n",
    "reward_step = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f3193c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vecinos\n",
    "def get_next_state(s, action):\n",
    "    row, col = divmod(s, grid_size)\n",
    "    \n",
    "    if action == 0 and row > 0:\n",
    "        row -= 1 # up\n",
    "    elif action == 1 and row < grid_size - 1:\n",
    "        row += 1 # down\n",
    "    elif action == 2 and col > 0:\n",
    "        col -= 1 # left\n",
    "    elif action == 3 and col < grid_size - 1:\n",
    "        col += 1 # right\n",
    "        \n",
    "    new_state = row * grid_size + col\n",
    "    \n",
    "    if new_state in obstacles:\n",
    "        return s\n",
    "    \n",
    "    return new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41143cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#matriz de transicion\n",
    "P = {s:{} for s in states}\n",
    "\n",
    "for s in states:\n",
    "    if s == goal:\n",
    "        continue\n",
    "    for a in actions:\n",
    "        next_state = get_next_state(s, a)\n",
    "        P[s][a] = {next_state: 1.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "430d7679",
   "metadata": {},
   "outputs": [],
   "source": [
    "#funcion de recompensa\n",
    "R = {s: {} for s in states}\n",
    "\n",
    "for s in states:\n",
    "    if s == goal:\n",
    "        continue\n",
    "    \n",
    "    for a in actions:\n",
    "        next_state = get_next_state(s, a)\n",
    "        \n",
    "        if a not in R[s]:\n",
    "            R[s][a] = {}\n",
    "            \n",
    "        if next_state == s and s in obstacles:\n",
    "            R[s][a][next_state] = reward_obstacle\n",
    "        elif next_state == goal:\n",
    "            R[s][a][next_state] = reward_goal\n",
    "        else:\n",
    "            R[s][a][next_state] = reward_step"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
